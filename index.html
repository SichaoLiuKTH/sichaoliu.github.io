<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Sichao Liu</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Sichao Liu 
                </p>
                <p> I am a research fellow of the <a href="https://www.vr.se/english.html">Swedish Research Council</a> which awarded me a three-year international grant. I am rotating at the University of Cambridge and École Polytechnique Fédérale de Lausanne (EPFL), with <a href="https://www.mrc-cbu.cam.ac.uk/people/alex.woolgar/">Alex Woolgar</a> and <a href="https://www.epfl.ch/labs/biorob/people/ijspeert/">Auke Ijspeert</a> on fusing AI and robotics for cutting-edge applications.</p> 

                </p>Previously, I was a Scientist in Robotics, Control and AI-vision at the Department of Robotics and Mechatronics, <a href="https://global.abb/group/en/technology/corporate-research-centers/sweden">ABB Corporate Research Center </a> in Sweden. I completed my PhD in AI for robotics at KTH Royal Institute of Technology, working with <a href="https://www.kth.se/profile/lihuiw">Lihui Wang</a>. Before my PhD, I was a master student in Mechanical Engineering and Computer Science and completed my bachelor's in Mathematics and Statistics.</p>
                <p style="text-align:center">
                  <a href="https://sites.google.com/view/sichao-liu/home">Homepage</a> &nbsp;/&nbsp;
                  <a href="mailto:Sichao.Liu@mrc-cbu.cam.ac.uk">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com.hk/citations?user=MPzgMS0AAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
				          <a href="https://se.linkedin.com/in/sichao-liu-a82813194">Linkedin</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/ErikLiuSe">Twitter</a> &nbsp;/&nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Sichao Liu.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Sichao Liu.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computer vision, generative AI, autonomous mobile robots and robotics foundation model. My research goal is to train robotics foundation models for general-purpose manipulation. Most of my research is about ulitilsing AI for robotic applications. </span>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	
    <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding:20px;width:35%;vertical-align:super">
        <div class="one">
          <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
          <source src="" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/cirp2024.png' width=150%>
        </div>
        <script type="text/javascript">
          function smerf_start() {
            document.getElementById('smerf_image').style.opacity = "1";
          }

          function smerf_stop() {
            document.getElementById('smerf_image').style.opacity = "0";
          }
          smerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://doi.org/10.1016/j.cirp.2024.03.004">
          <span class="papertitle">Vision AI-based human-robot collaborative assembly driven by autonomous robots</span>
        </a>
        <br>
        <strong>Sichao Liu</strong></a>,
        Jianjing Zhang</a>,
        Robert X. Gao</a>,
        Lihui Wang</a>
        <br>
        <em>CIRP Annals - Manufacturing Technology</em>, 2024
        <br>
        <a href="https://sichaoliukth.github.io/visonAI-LLM/">project page</a>
        /
        <a href="https://kth-my.sharepoint.com/:p:/g/personal/sicliu_ug_kth_se/EaytmypocPZIuEGc1siWEiEBiIOWdI62jFtdu2-PpPzRdA?e=RDfeHj">slide</a>
        /
        <a href="">code</a>
        <p></p>
        <p>
        We introduce a vision AI-based method for human-robot collaborative (HRC) assembly, enabled by a large language model (LLM). Upon 3D object reconstruction and pose establishment through neural object field modelling, a visual servoing-based mobile robotic system performs object manipulation and navigation guidance to a mobile robot. The LLM model provides text-based logic reasoning and high-level control command generation for natural human-robot interactions.
        </p>
      </td>
    </tr>

    <tr onmouseout="case_stop()" onmouseover="case_start()">
      <td style="padding:20px;width:35%;vertical-align:middle">
        <div class="one">
          <div class="two" id='case_image'><video  width=150% muted autoplay loop>
          <source src="images/Case2024.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/case2024.png' width=150%>
        </div>
        <script type="text/javascript">
          function case_start() {
            document.getElementById('case_image').style.opacity = "1";
          }

          function case_stop() {
            document.getElementById('case_image').style.opacity = "0";
          }
          case_stop()
        </script>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="">
          <span class="papertitle">Vision language model-driven scene understanding and object manipulation</span>
        </a>
        <br>
        <strong>Sichao Liu</strong></a>,
        Jianjing Zhang, Robert X. Gao, Lihui Wang</a>
        <br>
        <em>IEEE CASE</em>, 2024
        <br>
        <a href="https://kth-my.sharepoint.com/:p:/g/personal/sicliu_ug_kth_se/Ecwd-Ulpz-VAg2i_iLGvRS0Bgjq-_2nvACJEDfLtRKvGjA?e=J2kFZs">slide</a>
        /
        <a href="">code</a>
        <p></p>
        <p>
        We introduce a vision language model (VLM)-driven approach to 2D scene understanding of an unknown physical environment and object manipulation. Given language instructions, a pre-trained foundation model built on Llama-2 (7B) is adopted for image processing and scene understanding, by which feeding visual information into the model outputs its text description, and a zero shot-based approach to fine-grained visual grounding and object detection is developed to extract and localise objects of interest among tasks. Upon 3D reconstruction and pose estimate establishment of the object, a code-writing large language model (LLM) is adopted to generate high-level control codes and link language instructions with robot actions for downstream tasks.
      </p>
      </td>
    </tr>

    <tr onmouseout="VLM_stop()" onmouseover="VLM_start()">
      <td style="padding:20px;width:35%;vertical-align:middle">
        <div class="one">
          <div class="two" id='VLM_image'><video  width=150% muted autoplay loop>
          <source src="images/GPT+robot.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/robotskills.png' width=150%>
        </div>
        <script type="text/javascript">
          function VLM_start() {
            document.getElementById('VLM_image').style.opacity = "1";
          }

          function VLM_stop() {
            document.getElementById('VLM_image').style.opacity = "0";
          }
          VLM_stop()
        </script>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="">
          <span class="papertitle">Transferring robot skills to enhance general-purpose object manipulation</span>
        </a>
        <br>
        <strong>Sichao Liu</strong></a>
        <br>
        <em>In submission</em>, 2024
        <br>
        <a href="https://sichaoliukth.github.io/robotskills/">project page</a>
        /
        <a href="">code</a>
        <p></p>
        <p>
        We introduce a pose estimate approach to establish 6D pose of the objects with accurate bounding boxs and text lables, and it simultenouly build precise and robust pose estimate of multiple objects and has the capability of real-time tracking these poses. We use a visual servosing system to allow robots to measure object's pose in real-time and dynamically plan robot arm trajectory of approching the target. The developed approach can be seamlessly connected with LLMs, where natural lanuage instructions such as 'please grasp mayo', and then the robot can understand such instructions to grasp the object.  
        </p>
      </td>
    </tr>

    <tr onmouseout="aei_stop()" onmouseover="aei_start()">
      <td style="padding:20px;width:35%;vertical-align:middle">
        <div class="one">
          <div class="two" id='aei_image'><video  width=150% muted autoplay loop>
          <source src="images/AEI20241.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/aei.jpg' width=150%>
        </div>
        <script type="text/javascript">
          function aei_start() {
            document.getElementById('aei_image').style.opacity = "1";
          }

          function aei_stop() {
            document.getElementById('aei_image').style.opacity = "0";
          }
          aei_stop()
        </script>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://doi.org/10.1016/j.aei.2024.102371">
          <span class="papertitle">Safety-aware human-centric collaborative assembly</span>
        </a>
        <br>
        </a>Shuming Yi</a>, 
        <strong>Sichao Liu</strong></a>,
        </a>Yifan Yang, Sijie Yan, Daqiang Guo, Xi Vincent Wang, Lihui Wang</a>
        <br>
        <em>Advanced Engineering Informatics</em>, 2024
        <br>
        <a href="https://kth-my.sharepoint.com/:v:/g/personal/sicliu_ug_kth_se/EULetNh71KhLr0FECxVu8TQBuk4mir4G-z272zmzUiEazw?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJPbmVEcml2ZUZvckJ1c2luZXNzIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXciLCJyZWZlcnJhbFZpZXciOiJNeUZpbGVzTGlua0NvcHkifX0&e=zeRakr">video</a>
        /
        <a href="">code</a>
        <p></p>
        <p>
        We introduce a safety-aware human-centric collaborative assembly approach driven by function blocks, human action recognition for intention detection, and collision avoidance for safe robot control. A deep learning-based recognition system is developed for high-accuracy human intention recognition and prediction, and an assembly feature-based approach driven by function blocks is presented for assembly execution and control. Thus, assembly features and human behaviours during assembly are formulated to support safe assembly actions. Skeleton-based human behaviours are defined as control inputs to an adaptive safety-aware scheme. The scheme includes collaborative and parallel mode-based pre-warning and obstacle avoidance approaches for a human-centric collaborative assembly system.
        </p>
      </td>
    </tr>
	
    <tr onmouseout="rcim_stop()" onmouseover="rcim_start()">
      <td style="padding:20px;width:20%;vertical-align:auto">
        <div class="one">
          <div class="two" id='rcim_image'><video  width=100% muted autoplay loop>
          <source src="" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/cirp2021.jpg' width=150%>
        </div>
        <script type="text/javascript">
          function rcim_start() {
            document.getElementById('rcim_image').style.opacity = "1";
          }

          function rcim_stop() {
            document.getElementById('rcim_image').style.opacity = "0";
          }
          rcim_stop()
        </script>
      </td>
      <td style="padding:20px;width:8%;vertical-align:middle">
        <a href="https://doi.org/10.1016/j.rcim.2023.102610">
          <span class="papertitle">Cognitive neuroscience and robotics: Advancements and future research directions</span>
        </a>
        <br>
        <strong>Sichao Liu</strong></a>,
        </a> Lihui Wang, Robert X. Gao</a>
        <br>
        <em>Robotics and Computer-Integrated Manufacturing</em>, 2024
        <br>
        <a href="">paper</a>
        <p></p>
        <p>
        We provides a comprehensive review of the past and the current status at the intersection of robotics, neuroscience, and artificial intelligence and highlights future research directions. The advantages of this approach include (1) the direct haptic guidance of the robot by the worker with less effort, and (2) programming-free precise control capability for collaborative assembly operations with varying assembly tasks. This is facilitated by the accurate modelling of the robot dynamics in both the presliding and sliding regimes and an adaptive haptic control strategy.
        </p>
      </td>
    </tr>
    <tr onmouseout="rnerf_stop()" onmouseover="rnerf_start()">
      <td style="padding:20px;width:35%;vertical-align:middle">
        <div class="one">
          <div class="two" id='rnerf_image'><video  width=100% muted autoplay loop>
          <source src="" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/nerf2023.jpg' width=150%>
        </div>
        <script type="text/javascript">
          function rnerf_start() {
            document.getElementById('rnerf_image').style.opacity = "1";
          }

          function rnerf_stop() {
            document.getElementById('rnerf_image').style.opacity = "0";
          }
          rnerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://doi.org/10.1016/j.cirp.2023.04.013">
          <span class="papertitle">Neural rendering-enabled 3D modeling for rapid digitization of in-service products</span>
        </a>
        <br>
        </a>Jianjing Zhang</a>, 
        <strong>Sichao Liu</strong></a>,
        </a>Robert X. Gao, Lihui Wang</a>
        <br>
        <em>CIRP Annals - Manufacturing Technology</em>, 2023
        <br>
        <a href="https://kth-my.sharepoint.com/:p:/g/personal/sicliu_ug_kth_se/EebsndfToHdOhVo4KsjwFl8B-kHFW0BsSW2Isvo0E0KMZQ?e=k5OCME">slide</a>
        /
        <a href="">code</a>
        <p></p>
        <p>
        We introduce neural rendering as a novel method for rapid digital model building. It learns a radiance field from RGB images to determine the characteristics of the physical object. Textured mesh can be generated from the learned radiance field for efficient 3D modeling.
        </p>
      </td>
    </tr>

    <tr onmouseout="cirp2023_stop()" onmouseover="cirp2023_start()">
      <td style="padding:20px;width:35%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cirp2023_image'><video  width=100% muted autoplay loop>
          <source src="" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/cirp2022.jpg' width=150%>
        </div>
        <script type="text/javascript">
          function cirp2023_start() {
            document.getElementById('cirp2023_image').style.opacity = "1";
          }

          function cirp2023_stop() {
            document.getElementById('cirp2023_image').style.opacity = "0";
          }
          cirp2023_stop()
        </script>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://doi.org/10.1016/j.cirp.2022.03.024">
          <span class="papertitle">Digital twin-enabled advance execution for human-robot collaborative assembly</span>
        </a>
        <br>
        <strong>Sichao Liu</strong></a>,
        </a> Xi Vincent Wang, Lihui Wang</a>
        <br>
        <em>CIRP Annals - Manufacturing Technology</em>, 2022
        <br>
        <a href="https://kth-my.sharepoint.com/:p:/g/personal/sicliu_ug_kth_se/EUrclTGMc0dEmcxYmupe7KQBHnFdeUUUxUELgAza8JqvKQ?e=PO6jCx">slide</a>
        /
        <a href="https://kth-my.sharepoint.com/:v:/g/personal/sicliu_ug_kth_se/ESVLOBSp7ENLhfVPgL0sfm4BRGvNZVYx-mp4JH9CxXGzfQ?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJPbmVEcml2ZUZvckJ1c2luZXNzIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXciLCJyZWZlcnJhbFZpZXciOiJNeUZpbGVzTGlua0NvcHkifX0&e=XO5BWo">video</a>
        /
        <a href="">code</a>
        <p></p>
        <p>
        We investigate digital twin-driven human-robot collaborative assembly enabled by function blocks. Leveraging sensor data, digital models are developed to precisely mimic physical human-robot collaborative settings supported by a digital-twin architecture. An advance-execution twin system based on the current status through real-time condition monitoring performs assembly planning and adaptive robot control using a network of function blocks. An augmented reality-based interaction method using HoloLens further facilitates human-centric assembly.
       </p>
      </td>
    </tr>

    <tr onmouseout="asme2022_stop()" onmouseover="asme2022_start()">
      <td style="padding:20px;width:35%;vertical-align:middle">
        <div class="one">
          <div class="two" id='asme2022_image'><video  width=100% muted autoplay loop>
          <source src="" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/asme2022.png' width=150%>
        </div>
        <script type="text/javascript">
          function asme2022_start() {
            document.getElementById('asme2022_image').style.opacity = "1";
          }

          function asme2022_stop() {
            document.getElementById('asme2022_image').style.opacity = "0";
          }
          asme2022_stop()
        </script>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://doi.org/10.1115/1.4053806">
          <span class="papertitle">Multimodal Data-Driven Robot Control for Human-Robot Collaborative Assembly </span>
        </a>
        <br>
        <strong>Sichao Liu</strong></a>,
        </a>Lihui Wang, Xi Vincent Wang</a>
        <br>
        <em>Transactions of the ASME, Journal of Manufacturing Science and Engineering</em>, 2022
        <br>
        <a href="https://kth-my.sharepoint.com/:p:/g/personal/sicliu_ug_kth_se/ERt7nenOwTxGlYiTaGhmrNoBECGRbC6B_H-p-h5wjikhcw?e=C2DqDY">slide</a>
        /
        <a href="">code</a>
        <p></p>
        <p>
        We investigate multimodal data-driven robot control for human–robot collaborative assembly. Leveraging function blocks, a programming-free human–robot interface is designed to fuse multimodal human commands that accurately trigger defined robot control modalities. Deep learning is explored to develop a command classification system for low-latency and high-accuracy robot control, in which a spatial-temporal graph convolutional network is developed for a reliable and accurate translation of brainwave command phrases into robot commands. Then, multimodal data-driven high-level robot control during assembly is facilitated by the use of event-driven function blocks. The high-level commands serve as triggering events to algorithms execution of fine robot manipulation and assembly feature-based collaborative assembly.
      </p>
      </td>
    </tr>
    

    <tr onmouseout="cirp2021_stop()" onmouseover="cirp2021_start()">
      <td style="padding:20px;width:35%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cirp2021_image'><video  width=150% muted autoplay loop>
          <source src="images/brain.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/brain.jpg' width=150%>
        </div>
        <script type="text/javascript">
          function cirp2021_start() {
            document.getElementById('cirp2021_image').style.opacity = "1";
          }

          function cirp2021_stop() {
            document.getElementById('cirp2021_image').style.opacity = "0";
          }
          cirp2021_stop()
        </script>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://doi.org/10.1016/j.cirp.2021.04.091">
          <span class="papertitle">Function block-based human-robot collaborative assembly driven by brainwaves</span>
        </a>
        <br>
        </a>Lihui Wang</a>, 
        <strong>Sichao Liu</strong></a>,
        </a>Clatton Cooper, Xi Vincent Wang, Robert X. Gao</a>
        <br>
        <em>CIRP Annals - Manufacturing Technology</em>, 2022
        <br>
        <a href="https://kth-my.sharepoint.com/:p:/g/personal/sicliu_ug_kth_se/EaKfhzQ5Qk5Ehg7JeMjyEOwBqFmB3wEFUChFhoJCAfFNVQ?e=4jkC2e">slide</a>
        /
        <a href="https://kth-my.sharepoint.com/:v:/g/personal/sicliu_ug_kth_se/EUTAqAHKsTtKi5S7fOMUVG0BdWyivjpIhjxZmDI4QOmXXg?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJPbmVEcml2ZUZvckJ1c2luZXNzIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXciLCJyZWZlcnJhbFZpZXciOiJNeUZpbGVzTGlua0NvcHkifX0&e=ElJSNF">video</a>
        /
        <a href="">code</a>
        <p></p>
        <p>
        We investigates human-robot collaborative assembly based on function blocks and driven by brainwaves. Using wavelet transform, brainwaves measured by EEG sensors are converted to time-frequency images and subsequently classified by a convolutional neural network (CNN) as commands to trigger a network of function blocks for assembly actions.
      </p>
      </td>
    </tr>

    <tr onmouseout="rcim2021_stop()" onmouseover="rcim2021_start()">
      <td style="padding:20px;width:35%;vertical-align:middle">
        <div class="one">
          <div class="two" id='rcim2021_image'><video  width=150% muted autoplay loop>
          <source src="images/force-2.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/force-2.PNG' width=150%>
        </div>
        <script type="text/javascript">
          function rcim2021_start() {
            document.getElementById('rcim2021_image').style.opacity = "1";
          }

          function rcim2021_stop() {
            document.getElementById('rcim2021_image').style.opacity = "0";
          }
          rcim2021_stop()
        </script>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://doi.org/10.1016/j.rcim.2021.102168">
          <span class="papertitle">Sensorless force estimation for industrial robots using disturbance observer and neural learning of friction approximation</span>
        </a>
        <br>
        <strong>Sichao Liu</strong></a>,
        </a> Xi Vincent Wang, Lihui Wang</a>
        <br>
        <em>Robotics and Computer-Integrated Manufacturing</em>, 2021
        <br>
        <a href="https://doi.org/10.1016/j.rcim.2021.102168">paper</a>
        /
        <a href="">code</a>
        <p></p>
        <p>
        We introduce a sensorless scheme to estimate the unknown contact force induced by the physical interaction with robots, use NN learning of friction compensates model uncertainties with good approximation, and use disturbance Kalman filter observer to achieve accurate contact force estimation. Observers and NN learning of friction show robust observation to small contact force.</p>
      </td>
    </tr>

    <tr onmouseout="asme2021_stop()" onmouseover="asme2021_start()">
      <td style="padding:20px;width:35%;vertical-align:middle">
        <div class="one">
          <div class="two" id='asme2021_image'><video  width=100% muted autoplay loop>
          <source src="" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/asme2021.png' width=150%>
        </div>
        <script type="text/javascript">
          function asme2021_start() {
            document.getElementById('asme2021_image').style.opacity = "1";
          }

          function asme2021_stop() {
            document.getElementById('asme2021_image').style.opacity = "0";
          }
          asme2021_stop()
        </script>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href=" https://doi.org/10.1115/1.4050187">
          <span class="papertitle">Function block-based multimodal control for symbiotic human-robot collaborative assembly</span>
        </a>
        <br>
        <strong>Sichao Liu</strong></a>,
        </a>Xi Vincent Wang, Lihui Wang</a>
        <br>
        <em>Transactions of the ASME, Journal of Manufacturing Science and Engineering</em>, 2021
        <br>
        <a href="https://kth-my.sharepoint.com/:p:/g/personal/sicliu_ug_kth_se/ERt7nenOwTxGlYiTaGhmrNoBECGRbC6B_H-p-h5wjikhcw?e=C2DqDY">slide</a>
        /
        <a href="https://kth-my.sharepoint.com/:v:/g/personal/sicliu_ug_kth_se/EaXXfwQFGSlKqPY0Bzg119kBJZHaW8CX9pO-l8eE5llW4A?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJPbmVEcml2ZUZvckJ1c2luZXNzIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXciLCJyZWZlcnJhbFZpZXciOiJNeUZpbGVzTGlua0NvcHkifX0&e=ZEdCzO">video</a>
        /
        <a href="https://kth-my.sharepoint.com/:v:/g/personal/sicliu_ug_kth_se/EXDwwC7u6cBGpIEz7srCmGQB5KQKvvONYBjz6dSdIdLqBg?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJPbmVEcml2ZUZvckJ1c2luZXNzIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXciLCJyZWZlcnJhbFZpZXciOiJNeUZpbGVzTGlua0NvcHkifX0&e=FcXMPn">video</a>
        /
        <a href="https://kth-my.sharepoint.com/:v:/g/personal/sicliu_ug_kth_se/ESpVcGezYKVIi60GsGa75OcBm5p3NryyPquzdNNUYBCgDg?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJPbmVEcml2ZUZvckJ1c2luZXNzIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXciLCJyZWZlcnJhbFZpZXciOiJNeUZpbGVzTGlua0NvcHkifX0&e=RUjuWm">video</a>
        /
        <a href="">code</a>
        <p></p>
        <p>
        We introduce a novel function block-enabled multimodal control approach for symbiotic human–robot collaborative assembly. Within the context, event-driven function blocks as reusable functional modules embedded with smart algorithms are used for the encapsulation of assembly feature-based tasks/processes and control commands that are transferred to the controller of robots for execution. Then, multimodal control commands in the form of sensorless haptics, gestures, and voices serve as the inputs of the function blocks to trigger task execution and human-centered robot control within a safe human–robot collaborative environment.
      </p>
      </td>
    </tr>

    <tr onmouseout="cirpj2021_stop()" onmouseover="cirpj2021_start()">
      <td style="padding:20px;width:35%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cirpj2021_image'><video  width=150% muted autoplay loop>
          <source src="images/haptic.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/cirpj.png' width=150%>
        </div>
        <script type="text/javascript">
          function cirpj2021_start() {
            document.getElementById('cirpj2021_image').style.opacity = "1";
          }

          function cirpj2021_stop() {
            document.getElementById('cirpj2021_image').style.opacity = "0";
          }
          cirpj2021_stop()
        </script>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://doi.org/10.1016/j.cirpj.2020.11.015">
          <span class="papertitle">Sensorless haptic control for human-robot collaborative assembly</span>
        </a>
        <br>
        <strong>Sichao Liu</strong></a>,
        </a>Xi Vincent Wang, Lihui Wang</a>
        <br>
        <em>CIRP Journal of Manufacturing Science and Technology</em>, 2021
        <br>
        <a href="https://doi.org/10.1016/j.cirpj.2020.11.015">paper</a>
        <p></p>
        <p>
        We introduce an approach to haptically controlling an industrial robot without using any external sensors for human-robot collaborative assembly. The advantages of this approach include (1) the direct haptic guidance of the robot by the worker with less effort, and (2) programming-free precise control capability for collaborative assembly operations with varying assembly tasks. This is facilitated by the accurate modelling of the robot dynamics in both the presliding and sliding regimes and an adaptive haptic control strategy. 
      </p>
      </td>
    </tr>



        </td>
      </tr>
    </table>
  </body>
</html>
